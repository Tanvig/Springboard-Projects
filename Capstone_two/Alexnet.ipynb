{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    ">AlexNet was designed by Hinton, winner of the 2012 ImageNet competition, and his student Alex Krizhevsky. It was also after that year that more and deeper neural networks were proposed, such as the excellent vgg, GoogleLeNet. Its official data model has an accuracy rate of 57.1% and top 1-5 reaches 80.2%. This is already quite outstanding for traditional machine learning classification algorithms.\n",
    "\n",
    "\n",
    "![title](img/alexnet.png)\n",
    "\n",
    "\n",
    "![title](img/alexnet2.png)\n",
    "\n",
    ">The following table below explains the network structure of AlexNet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "<thead>\n",
    "\t<tr>\n",
    "\t\t<th>Size / Operation</th>\n",
    "\t\t<th>Filter</th>\n",
    "\t\t<th>Depth</th>\n",
    "\t\t<th>Stride</th>\n",
    "\t\t<th>Padding</th>\n",
    "\t\t<th>Number of Parameters</th>\n",
    "\t\t<th>Forward Computation</th>\n",
    "\t</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "\t<tr>\n",
    "\t\t<td>3* 227 * 227</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv1 + Relu</td>\n",
    "\t\t<td>11 * 11</td>\n",
    "\t\t<td>96</td>\n",
    "\t\t<td>4</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>(11*11*3 + 1) * 96=34944</td>\n",
    "\t\t<td>(11*11*3 + 1) * 96 * 55 * 55=105705600</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>96 * 55 * 55</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Max Pooling</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>96 * 27 * 27</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Norm</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv2 + Relu</td>\n",
    "\t\t<td>5 * 5</td>\n",
    "\t\t<td>256</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td>(5 * 5 * 96 + 1) * 256=614656</td>\n",
    "\t\t<td>(5 * 5 * 96 + 1) * 256 * 27 * 27=448084224</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>256 * 27 * 27</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Max Pooling</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>256 * 13 * 13</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Norm</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv3 + Relu</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td>384</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>(3 * 3 * 256 + 1) * 384=885120</td>\n",
    "\t\t<td>(3 * 3 * 256 + 1) * 384 * 13 * 13=149585280</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>384 * 13 * 13</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv4 + Relu</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td>384</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 384=1327488</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 384 * 13 * 13=224345472</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>384 * 13 * 13</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv5 + Relu</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td>256</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 256=884992</td>\n",
    "\t\t<td>(3 * 3 * 384 + 1) * 256 * 13 * 13=149563648</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>256 * 13 * 13</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Max Pooling</td>\n",
    "\t\t<td>3 * 3</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>2</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>256 * 6 * 6</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Dropout (rate 0.5)</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>FC6 + Relu</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>256 * 6 * 6 * 4096=37748736</td>\n",
    "\t\t<td>256 * 6 * 6 * 4096=37748736</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>4096</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Dropout (rate 0.5)</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>FC7 + Relu</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>4096 * 4096=16777216</td>\n",
    "\t\t<td>4096 * 4096=16777216</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>4096</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>FC8 + Relu</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>4096 * 1000=4096000</td>\n",
    "\t\t<td>4096 * 1000=4096000</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>1000 classes</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Overall</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>62369152=62.3 million</td>\n",
    "\t\t<td>1135906176=1.1 billion</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Conv VS FC</td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td></td>\n",
    "\t\t<td>Conv:3.7million (6%) , FC: 58.6 million  (94% )</td>\n",
    "\t\t<td>Conv: 1.08 billion (95%) , FC: 58.6 million (5%)</td>\n",
    "\t</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why does AlexNet achieve better results?\n",
    "\n",
    "1. **Relu activation function is used.**\n",
    "\n",
    "Relu function: f (x) = max (0, x)\n",
    "\n",
    "![alex1](img/alex512.png)\n",
    "\n",
    "ReLU-based deep convolutional networks are trained several times faster than tanh and sigmoid- based networks. The following figure shows the number of iterations for a four-layer convolutional network based on CIFAR-10 that reached 25% training error in tanh and ReLU:\n",
    "\n",
    "![alex1](img/alex612.png)\n",
    "\n",
    "2. **Standardization ( Local Response Normalization )**\n",
    "\n",
    "After using ReLU f (x) = max (0, x), you will find that the value after the activation function has no range like the tanh and sigmoid functions, so a normalization will usually be done after ReLU, and the LRU is a steady proposal (Not sure here, it should be proposed?) One method in neuroscience is called \"Lateral inhibition\", which talks about the effect of active neurons on its surrounding neurons.\n",
    "\n",
    "![alex1](img/alex3.jpg)\n",
    "\n",
    "\n",
    "3. **Dropout**\n",
    "\n",
    "Dropout is also a concept often said, which can effectively prevent overfitting of neural networks. Compared to the general linear model, a regular method is used to prevent the model from overfitting. In the neural network, Dropout is implemented by modifying the structure of the neural network itself. For a certain layer of neurons, randomly delete some neurons with a defined probability, while keeping the individuals of the input layer and output layer neurons unchanged, and then update the parameters according to the learning method of the neural network. In the next iteration, rerandom Remove some neurons until the end of training.\n",
    "\n",
    "\n",
    "![alex1](img/alex4.jpg)\n",
    "\n",
    "\n",
    "4. **Enhanced Data ( Data Augmentation )**\n",
    "\n",
    "\n",
    "\n",
    "**In deep learning, when the amount of data is not large enough, there are generally 4 solutions:**\n",
    "\n",
    ">  Data augmentation- artificially increase the size of the training set-create a batch of \"new\" data from existing data by means of translation, flipping, noise\n",
    "\n",
    ">  Regularization——The relatively small amount of data will cause the model to overfit, making the training error small and the test error particularly large. By adding a regular term after the Loss Function , the overfitting can be suppressed. The disadvantage is that a need is introduced Manually adjusted hyper-parameter.\n",
    "\n",
    ">  Dropout- also a regularization method. But different from the above, it is achieved by randomly setting the output of some neurons to zero\n",
    "\n",
    ">  Unsupervised Pre-training- use Auto-Encoder or RBM's convolution form to do unsupervised pre-training layer by layer, and finally add a classification layer to do supervised Fine-Tuning\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tflearn\n",
      "  Downloading tflearn-0.3.2.tar.gz (98 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages (from tflearn) (1.18.1)\n",
      "Requirement already satisfied: six in c:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages (from tflearn) (1.14.0)\n",
      "Collecting Pillow\n",
      "  Using cached Pillow-7.1.2-cp36-cp36m-win_amd64.whl (2.0 MB)\n",
      "Building wheels for collected packages: tflearn\n",
      "  Building wheel for tflearn (setup.py): started\n",
      "  Building wheel for tflearn (setup.py): finished with status 'done'\n",
      "  Created wheel for tflearn: filename=tflearn-0.3.2-py3-none-any.whl size=128212 sha256=baf9f3b49b41685519bbb2baef4e4558065639c709aec73093cca0856d9bd071\n",
      "  Stored in directory: c:\\users\\tanvi\\appdata\\local\\pip\\cache\\wheels\\14\\e9\\35\\ac682b1d18f932dc39cf86f25b22bb70df2e80e45851e4f9f3\n",
      "Successfully built tflearn\n",
      "Installing collected packages: Pillow, tflearn\n",
      "Successfully installed Pillow-7.1.2 tflearn-0.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tflearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-246171f88fdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# (2) Get Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moxflower17\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moxflower17\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moxflower17\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\tflearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_training_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\tflearn\\config.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# -------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\tflearn\\variables.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madd_arg_scope\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcontrib_add_arg_scope\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,\\\n",
    " Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "# (2) Get Data\n",
    "import tflearn.datasets.oxflower17 as oxflower17\n",
    "x, y = oxflower17.load_data(one_hot=True)\n",
    "\n",
    "# (3) Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),\\\n",
    " strides=(4,4), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling \n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation before passing it to the next layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Passing it to a dense layer\n",
    "model.add(Flatten())\n",
    "# 1st Dense Layer\n",
    "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Dense Layer\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Dense Layer\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(17))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# (4) Compile \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\\\n",
    " metrics=['accuracy'])\n",
    "\n",
    "# (5) Train\n",
    "model.fit(x, y, batch_size=64, epochs=1, verbose=1, \\\n",
    "validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 3 from 1 for 'conv2d_7/convolution' (op: 'Conv2D') with input shapes: [?,1,1,56], [3,3,56,384].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1618\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1619\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1620\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_7/convolution' (op: 'Conv2D') with input shapes: [?,1,1,56], [3,3,56,384].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b4f0809bd68b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# 3rd Convolutional Layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m# Batch Normalisation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[0;32m    172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m   3715\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3716\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf_data_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3717\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m   3718\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'channels_first'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtf_data_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'NHWC'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3719\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# NHWC -> NCHW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m       name=name)\n\u001b[0m\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution)\u001b[0m\n\u001b[0;32m   1008\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m   1011\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m    966\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m                   \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m                   data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[0;32m    969\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    740\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0;32m    741\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[0;32m    743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3320\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3321\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3322\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3323\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3324\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1784\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1785\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1786\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1787\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1788\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1620\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1621\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1622\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_7/convolution' (op: 'Conv2D') with input shapes: [?,1,1,56], [3,3,56,384]."
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,\\\n",
    " Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "# Loading the dataset and perform splitting\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Peforming reshaping operation\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Normalization\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# One Hot Encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# (3) Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters= 32, input_shape=(28,28,1), kernel_size=(5,5),\\\n",
    " strides=(4,4), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling \n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation before passing it to the next layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=56, kernel_size=(2,2), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Passing it to a dense layer\n",
    "model.add(Flatten())\n",
    "# 1st Dense Layer\n",
    "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Dense Layer\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Dense Layer\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# (4) Compile \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\\\n",
    " metrics=['accuracy'])\n",
    "\n",
    "# (5) Train\n",
    "# model.fit(x, y, batch_size=64, epochs=1, verbose=1, \\\n",
    "# validation_split=0.2, shuffle=True)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\ipykernel_launcher.py:54: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3072, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "c:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\ipykernel_launcher.py:59: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4096, activation=\"relu\", kernel_initializer=\"glorot_normal\")`\n",
      "c:\\users\\tanvi\\anaconda3\\envs\\lenetenv\\lib\\site-packages\\ipykernel_launcher.py:64: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, activation=\"softmax\", kernel_initializer=\"glorot_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 52963s 883ms/step - loss: 0.3943 - accuracy: 0.8655 - val_loss: 0.0795 - val_accuracy: 0.9808\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 15284s 255ms/step - loss: 0.0865 - accuracy: 0.9805 - val_loss: 0.0630 - val_accuracy: 0.9845\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 21457s 358ms/step - loss: 0.0657 - accuracy: 0.9852 - val_loss: 0.0779 - val_accuracy: 0.9824\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 53222s 887ms/step - loss: 0.0572 - accuracy: 0.9877 - val_loss: 0.0578 - val_accuracy: 0.9865\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 12582s 210ms/step - loss: 0.0531 - accuracy: 0.9885 - val_loss: 0.0479 - val_accuracy: 0.9885\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 11438s 191ms/step - loss: 0.0413 - accuracy: 0.9910 - val_loss: 0.0488 - val_accuracy: 0.9904\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 13250s 221ms/step - loss: 0.0447 - accuracy: 0.9898 - val_loss: 0.0454 - val_accuracy: 0.9896\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 11490s 191ms/step - loss: 0.0341 - accuracy: 0.9926 - val_loss: 0.0444 - val_accuracy: 0.9917\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 14058s 234ms/step - loss: 0.0289 - accuracy: 0.9937 - val_loss: 0.0547 - val_accuracy: 0.9905\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 14039s 234ms/step - loss: 0.0278 - accuracy: 0.9937 - val_loss: 0.0664 - val_accuracy: 0.9871\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 11667s 194ms/step - loss: 0.0251 - accuracy: 0.9945 - val_loss: 0.0831 - val_accuracy: 0.9910\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 11923s 199ms/step - loss: 0.0376 - accuracy: 0.9919 - val_loss: 0.0976 - val_accuracy: 0.9855\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 11385s 190ms/step - loss: 0.0285 - accuracy: 0.9940 - val_loss: 0.0609 - val_accuracy: 0.9907\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 13611s 227ms/step - loss: 0.0321 - accuracy: 0.9937 - val_loss: 0.0743 - val_accuracy: 0.9864\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 11094s 185ms/step - loss: 0.0256 - accuracy: 0.9946 - val_loss: 0.0833 - val_accuracy: 0.9910\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 11101s 185ms/step - loss: 0.0240 - accuracy: 0.9957 - val_loss: 0.0501 - val_accuracy: 0.9902\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 12112s 202ms/step - loss: 0.0156 - accuracy: 0.9964 - val_loss: 0.0696 - val_accuracy: 0.9908\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 11883s 198ms/step - loss: 0.0175 - accuracy: 0.9961 - val_loss: 0.0619 - val_accuracy: 0.9905\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 11692s 195ms/step - loss: 0.0318 - accuracy: 0.9944 - val_loss: 0.0606 - val_accuracy: 0.9878\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 11585s 193ms/step - loss: 0.0240 - accuracy: 0.9956 - val_loss: 0.0549 - val_accuracy: 0.9893\n",
      "10000/10000 [==============================] - 34s 3ms/step\n",
      "Test Loss: 0.05492070618461985\n",
      "Test accuracy: 0.989300012588501\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
    "\n",
    "# Loading the dataset and perform splitting\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Peforming reshaping operation\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Normalization\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# One Hot Encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(96, kernel_size = (11, 11), activation = \"relu\",input_shape = (28,28, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "          \n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv2D(256, kernel_size = (5, 5), activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 3\n",
    "\n",
    "model.add(Conv2D(512, kernel_size = (3, 3), activation = \"relu\", padding = \"same\"))\n",
    "\n",
    "# Layer 4\n",
    "\n",
    "model.add(Conv2D(1024, kernel_size = (3, 3), activation = \"relu\", padding = \"same\"))\n",
    "\n",
    "\n",
    "# Layer 5\n",
    "\n",
    "model.add(Conv2D(1024, kernel_size = (3, 3), activation = \"relu\", padding = \"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 6\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3072, init='glorot_normal', activation = \"relu\"))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Layer 7\n",
    "model.add(Dense(4096, init='glorot_normal', activation = \"relu\"))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Layer 8\n",
    "model.add(Dense(10, init='glorot_normal', activation = \"softmax\"))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
